{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "import threading\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/'\n",
    "chroma_db = 'vectors/'\n",
    "model_name = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....document_loaded.....\n",
      ".....document_splitter.....\n",
      ".....document_splitted.....\n",
      ".....document_embedded.....\n",
      ".....document_loaded_at_db.....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check path\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "if not os.path.exists(chroma_db):\n",
    "    os.makedirs(chroma_db)\n",
    "# Create a DirectoryLoader instance to load PDF documents\n",
    "documents = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            reader = PdfReader(filepath)\n",
    "            full_text = ''\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text\n",
    "            documents.append(Document(page_content=full_text, metadata={\"source\": filename}))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "print('.....document_loaded.....')\n",
    "\n",
    "# Initialize a text splitter to divide documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "print('.....document_splitter.....')\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print('.....document_splitted.....')\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2',\n",
    "                                  model_kwargs={'device': 'cpu'})\n",
    "print('.....document_embedded.....')\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings,persist_directory=chroma_db)\n",
    "print('.....document_loaded_at_db.....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Input question to LLM:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.similarity_search(question, k=3)\n",
    "context = \" \".join(doc.page_content for doc in results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI assistant in Astree and static testing. You are able to find answers to the questions from the contextual passage snippets provided and from your knowledge.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = f\"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{USER_PROMPT}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking= True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, let's see. The user asked about \"overflow deference\" and I need to use the provided context to answer. \n",
      "\n",
      "Looking at the context, there's a part where it mentions that in earlier versions of Astrée, overflow deference was handled by case analysis, which involved unfolding the array or partitioning the loop. More recently, there's a symbolic memory predicate domain that automatically resolves this without those methods. \n",
      "\n",
      "The question is about overflow deference, so I need to connect the context to this. The key point here is that earlier versions required specific handling, but newer versions use a symbolic domain to avoid it. The answer should mention that the symbolic domain solves the issue without manual handling, which is the overflow deference.\n",
      "</think>\n",
      "content: \n",
      "\n",
      "The overflow deference in Astrée is resolved by the symbolic memory predicate domain in more recent versions, which automatically handles the issue without requiring manual case analysis or unfolding. This allows for automatic resolution of overflow without explicit partitioning or unrolling of loops.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1200,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist() \n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
